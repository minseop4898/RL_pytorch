{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from saida_gym.starcraft_multi.marineVsZealot import MarineVsZealot\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 0.5 ## for exploration, annealed every episode\n",
    "LAMBDA = 0.8 # for TD lambda\n",
    "BATCH_SIZE = 10\n",
    "GAMMA = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(11, 256)\n",
    "        self.fc2 = nn.Linear(256,256)\n",
    "        self.fc_pi = nn.Linear(256, 15)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.0005)\n",
    "    \n",
    "    def pi(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        policy = (1-EPS)*F.softmax(self.fc_pi(x), dim=1) + (EPS/15) ## exploration\n",
    "        return policy\n",
    "    \n",
    "    def train(self, loss):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        self.optimizer.step()\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(51, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.q = nn.Linear(256, 15)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.0005)\n",
    "        \n",
    "    def Q(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        q = self.q(x)\n",
    "        return q\n",
    "    \n",
    "    def train(self, loss):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        self.optimizer.step()\n",
    "    \n",
    "class Make_data():\n",
    "    def __init__(self):\n",
    "        self.enemy_hp = 160.0\n",
    "        self.my_hp = 120.0\n",
    "        self.agents_state_list, self.global_state_list, self.action_list,\\\n",
    "        self.reward_list, self.done_list = [], [], [], [], []\n",
    "    \n",
    "    def feature_scale(self, hp, pos_x, pos_y, velo_x, velo_y, is_en):\n",
    "        if not is_en:\n",
    "            return [hp/10.0, pos_x/1000.0, pos_y/1000.0, velo_x, velo_y]\n",
    "        else:\n",
    "            return [hp/30.0, pos_x/1000.0, pos_y/1000.0]\n",
    "        \n",
    "    def make_agent_state(self, obs):\n",
    "        agent_list = []\n",
    "        en = obs.en_unit[0]\n",
    "        enemy_state = self.feature_scale(en.hp+en.shield, en.pos_x, en.pos_y, en.velocity_x, en.velocity_y, is_en=True)\n",
    "        for i, agent in enumerate(obs.my_unit):\n",
    "            hp, pos_x, pos_y, velo_x, velo_y = agent.hp, agent.pos_x, agent.pos_y, agent.velocity_x, agent.velocity_y\n",
    "            agent_state = self.feature_scale(hp, pos_x, pos_y, velo_x, velo_y, is_en=False) + enemy_state\n",
    "            one_hot = [0,0,0]\n",
    "            one_hot[i] += 1\n",
    "            agent_list.append(agent_state + one_hot)\n",
    "        return torch.tensor(agent_list, dtype=torch.float)\n",
    "    \n",
    "    def make_global_state(self, obs):\n",
    "        en = obs.en_unit[0]\n",
    "        global_state = self.feature_scale(en.hp+en.shield, en.pos_x, en.pos_y, en.velocity_x, en.velocity_y, is_en=True)\n",
    "        for i, agent in enumerate(obs.my_unit):\n",
    "            hp, pos_x, pos_y, velo_x, velo_y = agent.hp, agent.pos_x, agent.pos_y, agent.velocity_x, agent.velocity_y\n",
    "            global_state += self.feature_scale(hp, pos_x, pos_y, velo_x, velo_y, is_en=False)\n",
    "        return torch.tensor(global_state, dtype=torch.float)\n",
    "    \n",
    "    def make_reward(self, obs, done):\n",
    "        reward = None\n",
    "        en = obs.en_unit[0]\n",
    "        en_hp = en.hp+en.shield\n",
    "        agents_hp = 0.0\n",
    "        for my in obs.my_unit:\n",
    "            agents_hp += my.hp\n",
    "        if self.enemy_hp > en_hp:\n",
    "            self.enemy_hp = en_hp\n",
    "            if self.my_hp > agents_hp:\n",
    "                self.my_hp = agents_hp\n",
    "                reward = -1.0 # 때렸지만 맞았을때\n",
    "            else:\n",
    "                reward = 0.65 # 때리기만 했을때\n",
    "        else:\n",
    "            if self.my_hp > agents_hp:\n",
    "                self.my_hp = agents_hp\n",
    "                reward = -1.0 # 맞기만 했을때\n",
    "            else:\n",
    "                reward = 0.0 # 때리지도 않고 맞지도 않았을때\n",
    "        if done:\n",
    "            self.enemy_hp = 160.0\n",
    "            self.my_hp = 120.0\n",
    "        return reward\n",
    "    \n",
    "    def store_data(self, agents_state, global_state, actions, reward, done):\n",
    "        self.agents_state_list.append(agents_state)\n",
    "        self.global_state_list.append(global_state)\n",
    "        self.action_list.append(actions)\n",
    "        self.reward_list.append(reward)\n",
    "        self.done_list.append(done)\n",
    "        \n",
    "    def store_terminal_state(self, agents_state, global_state):\n",
    "        self.agents_state_list.append(agents_state)\n",
    "        self.global_state_list.append(global_state)\n",
    "        \n",
    "    def return_training_data(self):\n",
    "        episode = [self.agents_state_list, self.global_state_list, self.action_list, self.reward_list, self.done_list]\n",
    "        self.agents_state_list, self.global_state_list, self.action_list,\\\n",
    "        self.reward_list, self.done_list = [], [], [], [], []\n",
    "        return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MarineVsZealot(frames_per_step=5, action_type=0, move_angle=30, move_dist=3, verbose=0,\\\n",
    "                          local_speed=0, no_gui=False, auto_kill=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor()\n",
    "critic = Critic()\n",
    "Data = Make_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_lambda(Q, a, r):\n",
    "    \"\"\"Evaluate TD lambda in single for loop! :) (with dynamic programming)\"\"\"\n",
    "    next_Q = torch.cat([Q[1:].gather(1,a[1:,[0]]), torch.zeros(1,1).float()], dim=0) ## concat for terminal state value == 0 \n",
    "    td_lambda_temp = r + GAMMA*next_Q\n",
    "    td_lambda = td_lambda_temp\n",
    "    for t in range(1, r.shape[0]):\n",
    "        shift_td = torch.cat([td_lambda_temp[t:], torch.zeros(t,1).float()], dim=0)\n",
    "        td_lambda_temp = r + GAMMA*shift_td\n",
    "        td_lambda_temp[-t:] *= 0.0\n",
    "        td_lambda += (LAMBDA**t)*td_lambda_temp\n",
    "    td_lambda *= 1 - LAMBDA\n",
    "    return td_lambda\n",
    "\n",
    "def training(episode):\n",
    "    agent_state_list, global_state_list, action_list, reward_list, done_list = episode\n",
    "    \n",
    "    s = torch.cat(global_state_list, dim=0).reshape(-1,18)\n",
    "    a = torch.cat(action_list, dim=0).reshape(-1,3)\n",
    "    r = torch.tensor(reward_list, dtype=torch.float).reshape(-1,1)\n",
    "    agent_one_hot = F.one_hot(torch.arange(s.shape[0])%1, num_classes=3).float()\n",
    "    action_one_hot = F.one_hot(a[:,[1,2]], num_classes=15).reshape(-1,30).float()\n",
    "    s_q_input = torch.cat([action_one_hot, s, agent_one_hot], dim=1)\n",
    "    Q = critic.Q(s_q_input)\n",
    "    \n",
    "    td_lambda = TD_lambda(Q, a, r)\n",
    "    for t in reversed(range(Q.shape[0])):\n",
    "        temp_Q = critic.Q(s_q_input[t])\n",
    "        cur_Q = temp_Q[a[t,0]]\n",
    "        critic_loss = (td_lambda[t,0].detach() - cur_Q).pow(2)\n",
    "        critic.train(critic_loss)\n",
    "    \n",
    "    for agent in range(3):\n",
    "        agent_state = torch.cat(agent_state_list, dim=1)[agent,:].reshape(-1,11)\n",
    "        agent_one_hot = F.one_hot(torch.tensor(agent).repeat(s.shape[0]), num_classes=3).float()\n",
    "        action_one_hot = F.one_hot(a[:,[i for i in range(a.shape[1]) if not agent==i]], num_classes=15).reshape(-1,30).float()\n",
    "        s_q_input = torch.cat([action_one_hot, s, agent_one_hot], dim=1)\n",
    "        Q = critic.Q(s_q_input)\n",
    "        real_Q = Q.gather(1,a[:,[agent]])\n",
    "        pi = actor.pi(agent_state)\n",
    "        coma = real_Q - torch.sum(pi*Q, dim=1).reshape(-1,1) ##### COMA calculation !!!!!\n",
    "        actor_loss = -torch.log(pi.gather(1,a[:,[agent]]))*coma.detach()\n",
    "        for i in range(actor_loss.shape[0]):\n",
    "            actor.train(actor_loss[i,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reward_sum = 0.0\n",
    "reward_list = []\n",
    "\n",
    "for ep in range(100000):\n",
    "    if EPS > 0.1: EPS -= 0.0001 # annealing EPS\n",
    "    observation = env.reset()\n",
    "    while True:\n",
    "        agents_state = Data.make_agent_state(observation)\n",
    "        global_state = Data.make_global_state(observation)\n",
    "        pi = actor.pi(agents_state)\n",
    "        actions = Categorical(pi).sample()\n",
    "        observation, _, done, _ = env.step(actions.numpy())\n",
    "        reward = Data.make_reward(observation, done)\n",
    "        reward_sum += reward\n",
    "        Data.store_data(agents_state, global_state, actions, reward, done)\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    training(Data.return_training_data())\n",
    "    \n",
    "    if ep % 20 == 19:\n",
    "        print('Episode %d'%ep,', Reward mean : %f'%(reward_sum/20.0))\n",
    "        reward_list.append(reward_sum/20.0)\n",
    "        #plt.plot(reward_list)\n",
    "        #plt.show()\n",
    "        reward_sum = 0.0\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
